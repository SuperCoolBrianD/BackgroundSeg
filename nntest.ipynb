{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81a4fe31-da7c-4758-9b69-7d976e607bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models, transforms, utils\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "from PIL import Image\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b451b4-86a7-4943-b692-d024a224db12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dingb/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b4851bb-877b-4ede-9afd-b009a7db04e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3eab5be-ba1d-47c8-89e6-ee9874072523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total convolution layers: 17\n",
      "conv_layers\n"
     ]
    }
   ],
   "source": [
    "# we will save the conv layer weights in this list\n",
    "model_weights =[]\n",
    "#we will save the 49 conv layers in this list\n",
    "conv_layers = []\n",
    "# get all the model children as list\n",
    "model_children = list(model.children())\n",
    "#counter to keep count of the conv layers\n",
    "counter = 0\n",
    "#append all the conv layers and their respective wights to the list\n",
    "for i in range(len(model_children)):\n",
    "    if type(model_children[i]) == nn.Conv2d:\n",
    "        counter+=1\n",
    "        model_weights.append(model_children[i].weight)\n",
    "        conv_layers.append(model_children[i])\n",
    "    elif type(model_children[i]) == nn.Sequential:\n",
    "        for j in range(len(model_children[i])):\n",
    "            for child in model_children[i][j].children():\n",
    "                if type(child) == nn.Conv2d:\n",
    "                    counter+=1\n",
    "                    model_weights.append(child.weight)\n",
    "                    conv_layers.append(child)\n",
    "print(f\"Total convolution layers: {counter}\")\n",
    "print(\"conv_layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c6b74cf-3922-4009-9074-58657ae70cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import roi_align\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab9b2426-9cd8-4c49-93ee-5cf8925db48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98eee193-99e7-4f49-bb79-0e8674bcd631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 960, 1280])\n",
      "Image shape before: torch.Size([3, 960, 1280])\n",
      "Image shape after: torch.Size([1, 3, 960, 1280])\n"
     ]
    }
   ],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    # transforms.Resize((1280,720)),\n",
    "    # transforms.CenterCrop(1280),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "image = Image.open('dst/fisheye/test/0.jpg')\n",
    "image = preprocess(image)\n",
    "img_shape = image.shape\n",
    "print(img_shape)\n",
    "print(f\"Image shape before: {image.shape}\")\n",
    "image = image.unsqueeze(0)\n",
    "print(f\"Image shape after: {image.shape}\")\n",
    "image = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d26cfe60-3ae3-48a7-ad40-7accc2d42391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "torch.Size([1, 64, 480, 640])\n",
      "torch.Size([1, 64, 480, 640])\n",
      "torch.Size([1, 64, 480, 640])\n",
      "torch.Size([1, 64, 480, 640])\n",
      "torch.Size([1, 64, 480, 640])\n",
      "torch.Size([1, 128, 240, 320])\n",
      "torch.Size([1, 128, 240, 320])\n",
      "torch.Size([1, 128, 240, 320])\n",
      "torch.Size([1, 128, 240, 320])\n",
      "torch.Size([1, 256, 120, 160])\n",
      "torch.Size([1, 256, 120, 160])\n",
      "torch.Size([1, 256, 120, 160])\n",
      "torch.Size([1, 256, 120, 160])\n",
      "torch.Size([1, 512, 60, 80])\n",
      "torch.Size([1, 512, 60, 80])\n",
      "torch.Size([1, 512, 60, 80])\n",
      "torch.Size([1, 512, 60, 80])\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "names = []\n",
    "for layer in conv_layers[0:]:\n",
    "    image = layer(image)\n",
    "    outputs.append(image)\n",
    "    names.append(str(layer))\n",
    "print(len(outputs))\n",
    "#print feature_maps\n",
    "for feature_map in outputs:\n",
    "    print(feature_map.shape)\n",
    "feature = outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7888ed6d-b671-47e3-878e-49ce40678a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.7878e+04, -2.8151e+04, -2.5242e+04,  ..., -1.6008e+04,\n",
      "           -1.2243e+04, -5.5153e+03],\n",
      "          [-3.7528e+04, -5.9350e+04, -5.5575e+04,  ..., -3.7982e+04,\n",
      "           -2.9278e+04, -1.2860e+04],\n",
      "          [-4.4648e+04, -7.0546e+04, -6.5057e+04,  ..., -4.4404e+04,\n",
      "           -3.3986e+04, -1.4087e+04],\n",
      "          ...,\n",
      "          [-4.7554e+04, -7.5498e+04, -6.9030e+04,  ..., -4.8214e+04,\n",
      "           -3.7418e+04, -1.4694e+04],\n",
      "          [-3.9602e+04, -6.3697e+04, -5.9099e+04,  ..., -4.2570e+04,\n",
      "           -3.2815e+04, -1.2766e+04],\n",
      "          [-2.0347e+04, -3.2085e+04, -2.6925e+04,  ..., -1.4888e+04,\n",
      "           -1.1159e+04, -2.7693e+03]],\n",
      "\n",
      "         [[ 1.0409e+04,  1.8848e+04,  1.9404e+04,  ...,  1.5680e+04,\n",
      "            1.5500e+04,  7.9603e+03],\n",
      "          [ 5.8602e+03,  1.2681e+04,  1.0690e+04,  ...,  7.5118e+03,\n",
      "            1.0667e+04,  4.7745e+03],\n",
      "          [-6.5281e+03, -7.5516e+03, -1.4639e+04,  ..., -1.5791e+04,\n",
      "           -6.4263e+03, -4.5891e+03],\n",
      "          ...,\n",
      "          [-1.7306e+04, -2.5894e+04, -3.6100e+04,  ..., -4.8024e+04,\n",
      "           -2.9534e+04, -1.6418e+04],\n",
      "          [-1.7303e+04, -2.6769e+04, -3.4993e+04,  ..., -4.1764e+04,\n",
      "           -2.6705e+04, -1.4506e+04],\n",
      "          [-1.8090e+04, -2.8579e+04, -3.4352e+04,  ..., -3.6865e+04,\n",
      "           -2.5534e+04, -1.3617e+04]],\n",
      "\n",
      "         [[ 7.9781e+03,  5.3236e+03,  3.5941e+03,  ..., -3.2931e+03,\n",
      "           -2.8448e+03, -4.9948e+03],\n",
      "          [ 3.9940e+03, -1.9182e+02, -2.6713e+02,  ..., -8.3261e+03,\n",
      "           -5.8814e+03, -6.7165e+03],\n",
      "          [ 8.6151e+03,  8.9142e+03,  1.2797e+04,  ...,  5.3547e+03,\n",
      "            5.9887e+03,  6.4027e+02],\n",
      "          ...,\n",
      "          [ 7.8480e+03,  1.1062e+04,  1.9841e+04,  ...,  2.0533e+04,\n",
      "            1.8498e+04,  9.4055e+03],\n",
      "          [ 8.4076e+03,  1.5000e+04,  2.6116e+04,  ...,  3.0166e+04,\n",
      "            2.4981e+04,  1.3460e+04],\n",
      "          [ 1.3193e+03,  4.0586e+03,  1.0451e+04,  ...,  1.3379e+04,\n",
      "            1.0625e+04,  5.2363e+03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.2026e+04, -3.1274e+04, -3.5806e+04,  ..., -3.0914e+04,\n",
      "           -2.6377e+04, -1.3508e+04],\n",
      "          [-4.6334e+04, -6.7951e+04, -7.6595e+04,  ..., -6.4995e+04,\n",
      "           -5.3334e+04, -2.7891e+04],\n",
      "          [-5.5265e+04, -7.9426e+04, -8.6676e+04,  ..., -7.0528e+04,\n",
      "           -5.6129e+04, -2.8611e+04],\n",
      "          ...,\n",
      "          [-4.5515e+04, -5.9357e+04, -5.7882e+04,  ..., -2.9399e+04,\n",
      "           -2.3466e+04, -1.0951e+04],\n",
      "          [-3.2902e+04, -4.1215e+04, -3.7458e+04,  ..., -1.3095e+04,\n",
      "           -1.0484e+04, -4.0476e+03],\n",
      "          [-1.7445e+04, -1.9622e+04, -1.4468e+04,  ...,  1.5436e+03,\n",
      "            1.1644e+03,  1.2100e+03]],\n",
      "\n",
      "         [[ 5.6014e+03,  7.0652e+03,  6.6819e+03,  ...,  7.0672e+03,\n",
      "            3.6439e+03,  1.0699e+03],\n",
      "          [ 2.1770e+04,  3.2462e+04,  3.8269e+04,  ...,  3.9204e+04,\n",
      "            2.6898e+04,  1.2832e+04],\n",
      "          [ 3.2532e+04,  5.2836e+04,  6.7273e+04,  ...,  7.3381e+04,\n",
      "            5.4949e+04,  3.0144e+04],\n",
      "          ...,\n",
      "          [ 3.4381e+04,  5.6007e+04,  7.3119e+04,  ...,  7.6848e+04,\n",
      "            5.9076e+04,  3.2532e+04],\n",
      "          [ 3.0816e+04,  5.1218e+04,  6.8678e+04,  ...,  7.4969e+04,\n",
      "            5.8440e+04,  3.3485e+04],\n",
      "          [ 1.4737e+04,  2.4511e+04,  3.4440e+04,  ...,  3.9681e+04,\n",
      "            3.1181e+04,  1.8103e+04]],\n",
      "\n",
      "         [[-2.9515e+04, -4.6260e+04, -5.0788e+04,  ..., -3.3344e+04,\n",
      "           -2.0538e+04, -1.1823e+04],\n",
      "          [-3.1711e+04, -4.6416e+04, -4.5385e+04,  ..., -1.3736e+04,\n",
      "            1.5465e+02, -5.6542e+01],\n",
      "          [-2.7783e+04, -3.9056e+04, -3.5939e+04,  ...,  1.4086e+03,\n",
      "            1.5381e+04,  8.9083e+03],\n",
      "          ...,\n",
      "          [-2.3100e+04, -2.9863e+04, -2.4610e+04,  ...,  1.7371e+04,\n",
      "            2.7168e+04,  1.4843e+04],\n",
      "          [-1.2947e+04, -1.4574e+04, -9.9083e+03,  ...,  2.0886e+04,\n",
      "            2.6758e+04,  1.5004e+04],\n",
      "          [-3.0182e+02,  2.3636e+03,  5.9030e+03,  ...,  1.8834e+04,\n",
      "            1.9943e+04,  1.0824e+04]]]], device='cuda:0',\n",
      "       grad_fn=<CudnnConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da0c9915-1e2e-44c3-b879-0d449a9bbbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5)\n",
      "torch.Size([1, 512, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "box = [torch.tensor([[ 382., 790., 437., 866.]]).to(device)]\n",
    "w, l = 437-382, 866-790\n",
    "scale = feature.shape[2]/img_shape[1]\n",
    "l = l*scale\n",
    "w = w*scale\n",
    "output_size = (int(math.ceil(w)), int(math.ceil(l)))\n",
    "print(output_size)\n",
    "aligned_f = roi_align(feature, box, output_size= output_size, spatial_scale=scale, aligned=True)\n",
    "print(aligned_f.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f7742f9-dd4a-46ab-814b-0ff8145913ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchvision' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "torchvision.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650a55b-3309-4234-868b-21b686e4d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53a9977a-7e5e-4c48-a90d-5b9b7f8dd0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "input_image = Image.open('detection/326.jpg')\n",
    "# input_image = input_image.convert(\"RGB\")\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "# print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "# print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1641c56c-3f61-4df2-8ea1-2d913ad693bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([473, 600, 477, 726, 783], device='cuda:0')\n",
      "can opener 0.41975638270378113\n",
      "hook 0.07524648308753967\n",
      "carpenter's kit 0.02796514891088009\n",
      "plane 0.026483098044991493\n",
      "screw 0.026465121656656265\n"
     ]
    }
   ],
   "source": [
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "# Show top categories per image\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "print(top5_catid)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b175a4d5-4b00-425e-a504-f1b4aecc7e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backgroundseg",
   "language": "python",
   "name": "backgroundseg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
