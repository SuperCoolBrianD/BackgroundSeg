{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81a4fe31-da7c-4758-9b69-7d976e607bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models, transforms, utils\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "from PIL import Image\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06b451b4-86a7-4943-b692-d024a224db12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dingb/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "model.name = 'resnet18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b4851bb-877b-4ede-9afd-b009a7db04e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet18\n"
     ]
    }
   ],
   "source": [
    "print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3eab5be-ba1d-47c8-89e6-ee9874072523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total convolution layers: 17\n",
      "conv_layers\n"
     ]
    }
   ],
   "source": [
    "# we will save the conv layer weights in this list\n",
    "model_weights =[]\n",
    "#we will save the 49 conv layers in this list\n",
    "conv_layers = []\n",
    "# get all the model children as list\n",
    "model_children = list(model.children())\n",
    "#counter to keep count of the conv layers\n",
    "counter = 0\n",
    "#append all the conv layers and their respective wights to the list\n",
    "for i in range(len(model_children)):\n",
    "    if type(model_children[i]) == nn.Conv2d:\n",
    "        counter+=1\n",
    "        model_weights.append(model_children[i].weight)\n",
    "        conv_layers.append(model_children[i])\n",
    "    elif type(model_children[i]) == nn.Sequential:\n",
    "        for j in range(len(model_children[i])):\n",
    "            for child in model_children[i][j].children():\n",
    "                if type(child) == nn.Conv2d:\n",
    "                    counter+=1\n",
    "                    model_weights.append(child.weight)\n",
    "                    conv_layers.append(child)\n",
    "print(f\"Total convolution layers: {counter}\")\n",
    "print(\"conv_layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c6b74cf-3922-4009-9074-58657ae70cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import roi_align\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.models.feature_extraction import get_graph_node_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab9b2426-9cd8-4c49-93ee-5cf8925db48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c46b102-bde6-4dcf-88e1-ce8cc607b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes, eval_nodes = get_graph_node_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5fb2367-35fc-4885-80c8-3c1c6935ee46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc']\n"
     ]
    }
   ],
   "source": [
    "print(train_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0b6f06d-3825-4ec4-b77f-341b237bdf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'conv1', 'bn1', 'relu', 'maxpool', 'layer1.0.conv1', 'layer1.0.bn1', 'layer1.0.relu', 'layer1.0.conv2', 'layer1.0.bn2', 'layer1.0.add', 'layer1.0.relu_1', 'layer1.1.conv1', 'layer1.1.bn1', 'layer1.1.relu', 'layer1.1.conv2', 'layer1.1.bn2', 'layer1.1.add', 'layer1.1.relu_1', 'layer2.0.conv1', 'layer2.0.bn1', 'layer2.0.relu', 'layer2.0.conv2', 'layer2.0.bn2', 'layer2.0.downsample.0', 'layer2.0.downsample.1', 'layer2.0.add', 'layer2.0.relu_1', 'layer2.1.conv1', 'layer2.1.bn1', 'layer2.1.relu', 'layer2.1.conv2', 'layer2.1.bn2', 'layer2.1.add', 'layer2.1.relu_1', 'layer3.0.conv1', 'layer3.0.bn1', 'layer3.0.relu', 'layer3.0.conv2', 'layer3.0.bn2', 'layer3.0.downsample.0', 'layer3.0.downsample.1', 'layer3.0.add', 'layer3.0.relu_1', 'layer3.1.conv1', 'layer3.1.bn1', 'layer3.1.relu', 'layer3.1.conv2', 'layer3.1.bn2', 'layer3.1.add', 'layer3.1.relu_1', 'layer4.0.conv1', 'layer4.0.bn1', 'layer4.0.relu', 'layer4.0.conv2', 'layer4.0.bn2', 'layer4.0.downsample.0', 'layer4.0.downsample.1', 'layer4.0.add', 'layer4.0.relu_1', 'layer4.1.conv1', 'layer4.1.bn1', 'layer4.1.relu', 'layer4.1.conv2', 'layer4.1.bn2', 'layer4.1.add', 'layer4.1.relu_1', 'avgpool', 'flatten', 'fc']\n"
     ]
    }
   ],
   "source": [
    "print(eval_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "905588a5-916d-4e3a-8788-e33d522b40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_nodes = {\n",
    "    # node_name: user-specified key for output dict\n",
    "    'layer1.1.relu_1': 'layer1',\n",
    "    'layer2.1.relu_1': 'layer2',\n",
    "    'layer3.1.relu_1': 'layer3',\n",
    "    'layer4.1.relu_1': 'layer4',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "790d13e6-15ec-40ac-ab72-0cce0062e4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Module(\n",
       "    (0): Module(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Module(\n",
       "    (0): Module(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Module(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Module(\n",
       "    (0): Module(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Module(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Module(\n",
       "    (0): Module(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Module(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_feature_extractor(model, return_nodes=return_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98eee193-99e7-4f49-bb79-0e8674bcd631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 960, 1280])\n",
      "Image shape before: torch.Size([3, 960, 1280])\n",
      "Image shape after: torch.Size([1, 3, 960, 1280])\n"
     ]
    }
   ],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    # transforms.Resize((1280,720)),\n",
    "    # transforms.CenterCrop(1280),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "image = Image.open('dst/fisheye/test/0.jpg')\n",
    "image = preprocess(image)\n",
    "img_shape = image.shape\n",
    "print(img_shape)\n",
    "print(f\"Image shape before: {image.shape}\")\n",
    "image = image.unsqueeze(0)\n",
    "print(f\"Image shape after: {image.shape}\")\n",
    "image = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d26cfe60-3ae3-48a7-ad40-7accc2d42391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "torch.Size([1, 64, 480, 640])\n",
      "torch.Size([1, 64, 480, 640])\n",
      "torch.Size([1, 64, 480, 640])\n",
      "torch.Size([1, 64, 480, 640])\n",
      "torch.Size([1, 64, 480, 640])\n",
      "torch.Size([1, 128, 240, 320])\n",
      "torch.Size([1, 128, 240, 320])\n",
      "torch.Size([1, 128, 240, 320])\n",
      "torch.Size([1, 128, 240, 320])\n",
      "torch.Size([1, 256, 120, 160])\n",
      "torch.Size([1, 256, 120, 160])\n",
      "torch.Size([1, 256, 120, 160])\n",
      "torch.Size([1, 256, 120, 160])\n",
      "torch.Size([1, 512, 60, 80])\n",
      "torch.Size([1, 512, 60, 80])\n",
      "torch.Size([1, 512, 60, 80])\n",
      "torch.Size([1, 512, 60, 80])\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "names = []\n",
    "for layer in conv_layers[0:]:\n",
    "    image = layer(image)\n",
    "    outputs.append(image)\n",
    "    names.append(str(layer))\n",
    "print(len(outputs))\n",
    "#print feature_maps\n",
    "for feature_map in outputs:\n",
    "    print(feature_map.shape)\n",
    "feature = outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7888ed6d-b671-47e3-878e-49ce40678a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 60, 80])\n"
     ]
    }
   ],
   "source": [
    "print(feature.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da0c9915-1e2e-44c3-b879-0d449a9bbbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5)\n",
      "torch.Size([1, 512, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "box = [torch.tensor([[ 382., 790., 437., 866.]]).to(device)]\n",
    "w, l = 437-382, 866-790\n",
    "scale = feature.shape[2]/img_shape[1]\n",
    "l = l*scale\n",
    "w = w*scale\n",
    "output_size = (int(math.ceil(w)), int(math.ceil(l)))\n",
    "print(output_size)\n",
    "aligned_f = roi_align(feature, box, output_size= output_size, spatial_scale=scale, aligned=True)\n",
    "print(aligned_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5dbd70a-46cd-4f08-94ce-93eb1d4c49cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import AdaptiveMaxPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef353739-2d4c-41f6-8132-59f0ec6034a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = AdaptiveMaxPool2d((2, 2))\n",
    "out = m(aligned_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a74ed78b-ec38-47ea-98c0-2a7b67d4d78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 2, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "811509ff-ecec-4387-b04e-1c5f91a84363",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_f = torch.flatten(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fd8f45a-256c-4d93-8703-475fd03afe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_layer = nn.ReLU()\n",
    "o = relu_layer(final_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9a1cb85-d953-4526-9283-6488c357f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num class 2 for road object and not road object\n",
    "cls_score = nn.Linear(2048, 2).to(device)\n",
    "bbox_pred = nn.Linear(2048, 2 * 4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8198359-b177-47a3-802c-de52e61b4354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17058.2988,  7212.7549], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cls_score = cls_score(final_f)\n",
    "bbox_pred = bbox_pred(final_f)\n",
    "print(cls_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d65c2d5-ff85-4a38-bb39-41852e69ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-22822.5723,  13268.3457,  33149.7109, -19445.3262,  46201.9258,\n",
      "         28258.6973, -33579.6836, -20196.9043], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(bbox_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0620a842-bb16-4dab-96ee-13d3037f891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNNBase(nn.Module):\n",
    "    \"\"\"\n",
    "    Main class for Generalized R-CNN.\n",
    "\n",
    "    Arguments:\n",
    "        backbone (nn.Module):\n",
    "        rpn (nn.Module):\n",
    "        roi_heads (nn.Module): takes the features + the proposals from the RPN and computes\n",
    "            detections / masks from it.\n",
    "        transform (nn.Module): performs the data transformation from the inputs to feed into\n",
    "            the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone, rpn, roi_heads, transform):\n",
    "        super(FasterRCNNBase, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.backbone = backbone\n",
    "        self.rpn = rpn\n",
    "        self.roi_heads = roi_heads\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def eager_outputs(self, losses, detections):\n",
    "        if self.training:\n",
    "            return losses\n",
    "\n",
    "        return detections\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            images (list[Tensor]): images to be processed\n",
    "            targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional)\n",
    "\n",
    "        Returns:\n",
    "            result (list[BoxList] or dict[Tensor]): the output from the model.\n",
    "                During training, it returns a dict[Tensor] which contains the losses.\n",
    "                During testing, it returns list[BoxList] contains additional fields\n",
    "                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
    "\n",
    "        \"\"\"\n",
    "        if self.training and targets is None:\n",
    "            raise ValueError(\"In training mode, targets should be passed\")\n",
    "\n",
    "        if self.training:\n",
    "            assert targets is not None\n",
    "            for target in targets:\n",
    "                boxes = target[\"boxes\"]\n",
    "                if isinstance(boxes, torch.Tensor):\n",
    "                    if len(boxes.shape) != 2 or boxes.shape[-1] != 4:\n",
    "                        raise ValueError(\"Expected target boxes to be a tensor\"\n",
    "                                         \"of shape [N, 4], got {:}.\".format(\n",
    "                            boxes.shape))\n",
    "                else:\n",
    "                    raise ValueError(\"Expected target boxes to be of type \"\n",
    "                                     \"Tensor, got {:}.\".format(type(boxes)))\n",
    "\n",
    "        original_image_sizes = torch.jit.annotate(List[Tuple[int, int]], [])\n",
    "        for img in images:\n",
    "            val = img.shape[-2:]\n",
    "            assert len(val) == 2\n",
    "            original_image_sizes.append((val[0], val[1]))\n",
    "\n",
    "        images, targets = self.transform(images, targets)\n",
    "\n",
    "        features = self.backbone(images.tensors)\n",
    "        if isinstance(features, torch.Tensor):\n",
    "            features = OrderedDict([('0', features)])\n",
    "\n",
    "        proposals, proposal_losses = self.rpn(images, features, targets)\n",
    "\n",
    "        detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)\n",
    "\n",
    "        detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)\n",
    "\n",
    "        losses = {}\n",
    "        losses.update(detector_losses)\n",
    "        losses.update(proposal_losses)\n",
    "\n",
    "        return self.eager_outputs(losses, detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "189507e1-c120-4b35-a457-0e903113c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastRCNN_GMM(nn.Module):\n",
    "    def __init__(self, backbone, num_class):\n",
    "        super(FastRCNN_GMM, self).__init__()\n",
    "        if backbone.name == 'resnet18':\n",
    "            return_nodes = {\n",
    "                # node_name: user-specified key for output dict\n",
    "                'layer1.1.relu_1': 'layer1',\n",
    "                'layer2.1.relu_1': 'layer2',\n",
    "                'layer3.1.relu_1': 'layer3',\n",
    "                'layer4.1.relu_1': 'layer4',\n",
    "                }\n",
    "            self.backbone = create_feature_extractor(model, return_nodes=return_nodes)\n",
    "        cls_score = nn.Linear(2048, 2)\n",
    "        bbox_pred = nn.Linear(2048, 2 * 4)\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de7bd3ec-190e-49c0-b320-719cb733dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = FastRCNN_GMM(model, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4600b6bb-4957-4c44-ab6b-7ac4e3928e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    # transforms.Resize((1280,720)),\n",
    "    # transforms.CenterCrop(1280),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "image = Image.open('dst/fisheye/test/0.jpg')\n",
    "image = preprocess(image)\n",
    "image = image.unsqueeze(0)\n",
    "image = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37cf9e5a-2dc2-4639-b88f-642660efd351",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = m(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b1af5ae3-9c66-4bdd-b6ac-9becb1cec07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 30, 40])\n"
     ]
    }
   ],
   "source": [
    "print(o['layer4'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f7742f9-dd4a-46ab-814b-0ff8145913ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchvision' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "torchvision.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20da15a2-7ca4-461c-8550-09896e03f7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer1': '0', 'layer2': '1', 'layer3': '2', 'layer4': '3'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{f'layer{k}': str(v) for v, k in enumerate([1, 2, 3, 4])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650a55b-3309-4234-868b-21b686e4d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53a9977a-7e5e-4c48-a90d-5b9b7f8dd0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "input_image = Image.open('detection/326.jpg')\n",
    "# input_image = input_image.convert(\"RGB\")\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "# print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "# print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1641c56c-3f61-4df2-8ea1-2d913ad693bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([473, 600, 477, 726, 783], device='cuda:0')\n",
      "can opener 0.41975638270378113\n",
      "hook 0.07524648308753967\n",
      "carpenter's kit 0.02796514891088009\n",
      "plane 0.026483098044991493\n",
      "screw 0.026465121656656265\n"
     ]
    }
   ],
   "source": [
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "# Show top categories per image\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "print(top5_catid)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b175a4d5-4b00-425e-a504-f1b4aecc7e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backgroundseg",
   "language": "python",
   "name": "backgroundseg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
